# 6 爬虫

### 写在前面：所有代码已经上传至github，本地保存数据的链接如下：https://github.com/fantasyblue5/6Jotang
### 其中爬虫文件为robotjd.py，爬到的数据保存在JDcomments.txt中
### 两个进阶任务链接为：https://github.com/fantasyblue5/6Jotangmysql

### 感谢出题人子秋学长对我提出的问题进行耐心的解答！如果有机会进入工作室，一定请你喝奶茶！(●'◡'●)
## 基本任务-爬取链接中的评论
### 以下为尝试爬取步骤：
### 1.scrapy环境配置
* 根据学长的建议，选择了scrapy框架进行爬取，使用pycharm进行开发
* 安装scrapy
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sc1.png)
* 创建项目
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sc2.png)
* 创建一个小爬虫
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sc3.png)
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sc4.png)
* 在settings中将协议改为False
#### 得到了如下的爬虫
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sc5.png)
### 2.拿到url
* 简单学习爬虫基本知识后，来到要求爬取的网页（出题的学长心好狠，为什么不能换个好爬的网站(落泪.jpg))
* 网页本身的链接并不是我们需要的url，我们需要打开网页检查，在筛选器中搜索json，找到产品页真正的url，如图：
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/url.png)
* 打开评论下一页，重复上述步骤，发现所有url中存在某种规律，即每次更改的都是page后面的数字，从0开始。所以在代码中，我将url分为三段，便于多页爬取
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/url2.png)
* 了解到京东的反爬机制，在网页检查中找到headers所需要的头文件（第一次只用了UA和referer，还是被发现了拿不到数据，所以干脆都粘过来了）
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/url3.png)
### 3.请求访问
* 这里就实现了多页访问，因为担心拿到无效数据，所以把页数设置的多一点，每页10条，最后拿到了700条数据

![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/req1.png)

### 4.数据解析
* 我们点开刚才的url，看到一个乱七八糟的页面
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/url6.png)
* 最开始我本来打算用xpath方式解析数据，但是反复确认路径就是拿不到数据，我怀疑是我看到的路径和实际路径不一致，看到这个乱七八糟的页面，我不禁陷入沉思：要不试试json？
* 于是有了如下代码：
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/json.png)
* 最开始，即使没有语法错误，也拿不到数据，查找好多资料后发现，原来前面有20个字符不是解析的内容！(好坑！)
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/json2.png)
* 修改代码跳过这20个字符和2个括号之后，就能拿到解析后数据了
### 5.封装数据
* 在解析后的数据中，找到我们需要的内容，用户昵称对应'nickname',评论内容对应'content',评论时间对应'creationTime'
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/comf.png)
* 在settings中开启pipeline,同时在item中完成设置
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/field.png)
* pipeline中写入数据
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/pp.png)
* 爬虫中的代码
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/item.png)
* 在终端中输入scrapy crawl robotjd就爬起来了！
### 6.拿到所有的数据……了吗？
* 当我的爬虫终于跑起来了，我的心情十分激动，在我倒杯水的时间，它就突然停住了……
* 停住了……可是我还没有500条数据呢……
* 经过我努力的查找，我欣喜地发现，我的ip被封了……
* 此时，一个想法出现了：科学上网！
* 我将科学上网换了一个香港节点（之前用的是美国），然后换了一个新的ip，好耶！爬虫又跑起来了！
* 最后终于拿到了全部数据！ヽ(✿ﾟ▽ﾟ)ノ
## 进阶任务-将爬取到的数据存入数据库
### (一)配置数据库
### 1.进入MySql官网下载8.0.30版本，得到一个压缩文件
### 2.配置并初始化MySql
* 创建一个txt文档，输入如下代码，其中安装目录为下载目录，存放目录为新建的data文件夹目录
  
```
[mysqld]
# 设置3306端口
port=3306
# 设置mysql的安装目录   
basedir=D:\mysql-8.0.30-winx64
# 设置mysql数据库的数据的存放目录  
datadir=D:\mysql-8.0.30-winx64\data
# 允许最大连接数
max_connections=200
# 允许连接失败的次数。
max_connect_errors=10
# 服务端使用的字符集默认为utf8mb4
character-set-server=utf8mb4
# 创建新表时将使用的默认存储引擎
default-storage-engine=INNODB
# 默认使用“mysql_native_password”插件认证
#mysql_native_password
default_authentication_plugin=mysql_native_password
[mysql]
# 设置mysql客户端默认字符集
default-character-set=utf8mb4
[client]
# 设置mysql客户端连接服务端时默认使用的端口
port=3306
default-character-set=utf8mb4
```

* 将文档另存为“所有文件”类型，修改文件名为“my.ini”
![mysql1](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/mysql1.png)
* 使用管理员身份运行cmd，输入如下代码：

#### 注意：这里一定要用管理员身份，否则后续会出现无法安装MySql的情况（踩坑人落泪）
* 在bin文件路径下输入初始化命令，记录生成的初始密码
```
mysqld --initialize --console
```
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sql2.png)

* 输入安装命令
```
mysqld --install
```
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sql3.png)

* 启动MySql
```
net start mysql
```

* 再输入如下命令，输入刚才的初始密码
```
mysql -uroot -p
```


* 更改密码的方式如下：
```
ALTER USER 'root'@'localhost' IDENTIFIED BY '新的密码';
```
### 3.配置环境变量
* 在高级系统设置中新建一个环境变量
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sql4.png)
* 变量值为安装路径
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sql5.png)
* 随后在系统变量中的path新建加入如下字样：
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sql6.png)
### 4.检验安装是否成功
* 在cmd中bin路径下输入代码，登录数据库，查看信息：
```
mysql -h localhost -u root -p
```
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/sql7.png)
* 简单的语句测试：
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/database.png)

### (二)小白的学习笔记：
* 笔记使用markdown，已上传至github，链接如下：https://github.com/fantasyblue5/Note-for-MySql
### (三)将爬虫数据导入数据库：
* 由于爬虫的时候直接写成了存为txt格式，考虑到存成excel才可以直接在数据库中操作，所以选择在代码中连接数据库
* 爬虫文件不需修改，只需修改pipeline中文件
* 1.在数据库中建表，使用
```
create database JDcomments;
```
```
create table jdcoms(
    user varchar(50),
    content varchar(100),
    time datetime
)
```

* 2.建立数据库与爬虫文件的连接，在这里配置好数据库
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/QQ图片20220923205606.png)
* 注：由于我不是用本机ip运行爬虫，数据库权限似乎不允许这样，于是更新root用户的主机访问为任何值，输入如下代码：
```
update user set host='%'where user ='root'
```
* 3.连接
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/curse.png)
* 6..最后可以看到写入的数据了
![](https://cdn.jsdelivr.net/gh/fantasyblue5/Jotang-img@main/img/QQ图片20220923204708.jpg)

## 进阶任务-计算机网络的学习笔记
### 已上传github，链接为https://github.com/fantasyblue5/6Jotangmysql


